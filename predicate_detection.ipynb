{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predicate_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "z-1AkA6mo_WA"
      ],
      "authorship_tag": "ABX9TyNRa0cEWcLvQWkHZwzrhdS7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sigeek/Visual-Relationship-Detection-LTN/blob/master/predicate_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-1AkA6mo_WA"
      },
      "source": [
        "# Installation, create files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LHsSYva0hFc4",
        "outputId": "8425e2de-125b-4388-ae24-c7a7a7f46ebe"
      },
      "source": [
        "!pip install anytree\n",
        "!pip install rdflib\n",
        "!pip install tensorflow==1.15.5"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting anytree\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/65/be23d8c3ecd68d40541d49812cd94ed0f3ee37eb88669ca15df0e43daed1/anytree-2.8.0-py2.py3-none-any.whl (41kB)\n",
            "\r\u001b[K     |███████▉                        | 10kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 20kB 19.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 30kB 23.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 40kB 21.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from anytree) (1.15.0)\n",
            "Installing collected packages: anytree\n",
            "Successfully installed anytree-2.8.0\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 22.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib) (2.4.7)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rdflib) (1.15.0)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.0 rdflib-5.0.0\n",
            "Collecting tensorflow==1.15.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/51/99abd43185d94adaaaddf8f44a80c418a91977924a7bc39b8dacd0c495b0/tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 87kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.12.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.36.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.8.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
            "\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 502kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 47.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.12.1)\n",
            "Requirement already satisfied: h5py<=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (2.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.32.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.5) (56.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.4.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=ad31f420ebce9dfcb1cb41692db29bec395fffd324a897c1f0f0171c6a86bb35\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, keras-applications, gast, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 numpy-1.18.5 tensorboard-1.15.0 tensorflow-1.15.5 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Sy_xzhLJMGm"
      },
      "source": [
        "import json\n",
        "import csv\n",
        "input_file = \"data/sg_dataset/annotations_train.json\"\n",
        "output_file = 'data/sg_dataset/predicates_train.csv'\n",
        "\n",
        "f = open(input_file,)\n",
        "data = json.load(f)\n",
        "with open(output_file, 'w', newline='') as file:\n",
        "  writer = csv.writer(file) \n",
        "  for key in data:\n",
        "    value = data[key]\n",
        "    for val in value:\n",
        "      row = str(val['subject']['category']) + \",\" + str(val['predicate']) + \",\" + str(val['object']['category'])\n",
        "      writer.writerow([val['subject']['category'] , val['predicate'], val['object']['category'] ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPoNgfCwcJk2"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia5Gm2yAUcD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b91619-306f-44b2-e206-cd77128d05f2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYr-6IVtcPPR",
        "outputId": "07cd9b29-778e-40a6-9327-453e6f0faf9b"
      },
      "source": [
        "%cd '/content/drive/MyDrive/tesi/Visual-Relationship-Detection-LTN'\n",
        "\n",
        "# TODO creare script per scaricare dataset e piazzarlo nel posto giusto\n",
        "#!unzip 'data/sg_dataset.zip' -d 'data'\n",
        "\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/tesi/Visual-Relationship-Detection-LTN\n",
            "data\t\t\t   refine_predictions.py\n",
            "logictensornetworks.py\t   relationship_phase_detection.ipynb\n",
            "models\t\t\t   relationship_phrase_detection.py\n",
            "predicate_detection.ipynb  train.py\n",
            "predicate_detection.py\t   visual_relationship_dataset.py\n",
            "__pycache__\t\t   Visual-Relationship-Detection-master\n",
            "README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lxyAJv4cZ7n"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOUOArhvcp8u",
        "outputId": "fec740cf-a085-43e2-ee42-d4c85672c592"
      },
      "source": [
        "from visual_relationship_dataset import *\n",
        "import os\n",
        "import scipy.io as sio\n",
        "from PIL import Image\n",
        "import copy\n",
        "from refine_predictions import refine_equiv\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "\n",
        "img_dir = 'data/sg_dataset/sg_test_images'\n",
        "\n",
        "# Load training data for prio statistics on the dataset\n",
        "triples_of_train_data = get_data(\"train\", False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "End of loading data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlQOmT09TBNv"
      },
      "source": [
        "#idxs_of_positive_examples_of_predicates = {}\n",
        "\n",
        "#for predicate in selected_predicates:\n",
        "#    idxs_of_positive_examples_of_predicates[predicate] = np.where(np.array(predicates[triples_of_train_data[:, -2]]) == predicate)[0]\n",
        "\n",
        "#prior_stats = np.array([len(idxs_of_positive_examples_of_predicates[pred]) for pred in selected_predicates])\n",
        "#prior_freq = np.true_divide(prior_stats, np.sum(prior_stats))\n",
        "\n",
        "sum_predicates = len(triples_of_train_data[:, -2])\n",
        "predicates_annotations = triples_of_train_data[:, -2]\n",
        "from collections import Counter\n",
        "counter_values = Counter(predicates_annotations).values()\n",
        "\n",
        "prior_freq = np.zeros(70)\n",
        "for i, value in enumerate(counter_values):\n",
        "  prior_freq[i] = Counter(predicates_annotations)[i] / sum_predicates\n",
        "\n",
        "image_path = sio.loadmat('Visual-Relationship-Detection-master/data/imagePath.mat')\n",
        "gt = sio.loadmat('Visual-Relationship-Detection-master/evaluation/gt.mat')\n",
        "gt_sub_bboxes = gt['gt_sub_bboxes']\n",
        "gt_obj_bboxes = gt['gt_obj_bboxes']\n",
        "gt_tuple_label = gt['gt_tuple_label']"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q20b-EkbHTgw"
      },
      "source": [
        "for img_id in range(len(gt_sub_bboxes[0])):\n",
        "    if len(gt_sub_bboxes[0][img_id]) > 0:\n",
        "        assert np.all(gt_sub_bboxes[0][img_id][:, 0] < gt_sub_bboxes[0][img_id][:, 2])\n",
        "        assert np.all(gt_sub_bboxes[0][img_id][:, 1] < gt_sub_bboxes[0][img_id][:, 3])\n",
        "\n",
        "        assert np.all(gt_obj_bboxes[0][img_id][:, 0] < gt_obj_bboxes[0][img_id][:, 2])\n",
        "        assert np.all(gt_obj_bboxes[0][img_id][:, 1] < gt_obj_bboxes[0][img_id][:, 3])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkAiQ2-fL_F3"
      },
      "source": [
        "Cambiare 2743361338_3d1a1e3b93_o.png in .jpg\n",
        "Cambiare 4392556686_44d71ff5a0_o.gif in .jpg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4IIrPiAImxT"
      },
      "source": [
        "features_detected_bb = []\n",
        "obj_bboxes_ours = []\n",
        "sub_bboxes_ours = []\n",
        "obj_labels_ours = []\n",
        "sub_labels_ours = []\n",
        "\n",
        "semantic_feat_vect = np.zeros(len(types))\n",
        "\n",
        "for pic_idx in range(gt_tuple_label.shape[1]):\n",
        "    gt_sub_bboxes[0, pic_idx] = gt_sub_bboxes[0, pic_idx].astype(np.float)\n",
        "    gt_obj_bboxes[0, pic_idx] = gt_obj_bboxes[0, pic_idx].astype(np.float)\n",
        "    features_per_image = np.empty((0, 2*number_of_features + number_of_extra_features))\n",
        "    obj_bboxes_ours_per_image = np.array([]).reshape(0, 4)\n",
        "    sub_bboxes_ours_per_image = np.array([]).reshape(0, 4)\n",
        "    obj_label_per_image = np.array([])\n",
        "    sub_label_per_image = np.array([])\n",
        "\n",
        "    # normalize data\n",
        "    if len(gt_sub_bboxes[0, pic_idx]) > 0:\n",
        "        img = Image.open(os.path.join(img_dir, image_path['imagePath'][0, pic_idx][0]).replace('png', 'jpg'))\n",
        "        width, height = img.size\n",
        "        normalized_gt_sub_bboxes = copy.deepcopy(gt_sub_bboxes[0, pic_idx])\n",
        "        normalized_gt_sub_bboxes[:, -4] /= width\n",
        "        normalized_gt_sub_bboxes[:, -3] /= height\n",
        "        normalized_gt_sub_bboxes[:, -2] /= width\n",
        "        normalized_gt_sub_bboxes[:, -1] /= height\n",
        "\n",
        "        normalized_gt_obj_bboxes = copy.deepcopy(gt_obj_bboxes[0, pic_idx])\n",
        "        normalized_gt_obj_bboxes[:, -4] /= width\n",
        "        normalized_gt_obj_bboxes[:, -3] /= height\n",
        "        normalized_gt_obj_bboxes[:, -2] /= width\n",
        "        normalized_gt_obj_bboxes[:, -1] /= height\n",
        "\n",
        "    for bb_idx in range(len(gt_tuple_label[0, pic_idx])):\n",
        "        bb1 = normalized_gt_sub_bboxes[bb_idx]\n",
        "        bb2 = normalized_gt_obj_bboxes[bb_idx]\n",
        "        sub_label_per_image = np.append(sub_label_per_image, gt_tuple_label[0, pic_idx][bb_idx, 0])\n",
        "        obj_label_per_image = np.append(obj_label_per_image, gt_tuple_label[0, pic_idx][bb_idx, 2])\n",
        "\n",
        "        feat_vect_bb1 = np.hstack((semantic_feat_vect, bb1))\n",
        "        feat_vect_bb2 = np.hstack((semantic_feat_vect, bb2))\n",
        "        feat_vect_bb1[gt_tuple_label[0, pic_idx][bb_idx, 0]] = 1.0\n",
        "        feat_vect_bb2[gt_tuple_label[0, pic_idx][bb_idx, 2]] = 1.0\n",
        "        feat_vec_pair = np.hstack((feat_vect_bb1, feat_vect_bb2, computing_extended_features(bb1, bb2)))\n",
        "\n",
        "        features_per_image = np.vstack((features_per_image, feat_vec_pair[np.newaxis, :]))\n",
        "        sub_bboxes_ours_per_image = np.vstack((sub_bboxes_ours_per_image, gt_sub_bboxes[0, pic_idx][bb_idx]))\n",
        "        obj_bboxes_ours_per_image = np.vstack((obj_bboxes_ours_per_image, gt_obj_bboxes[0, pic_idx][bb_idx]))\n",
        "\n",
        "    features_detected_bb.append(features_per_image)\n",
        "    obj_bboxes_ours.append(obj_bboxes_ours_per_image)\n",
        "    sub_bboxes_ours.append(sub_bboxes_ours_per_image)\n",
        "    obj_labels_ours.append(obj_label_per_image)\n",
        "    sub_labels_ours.append(sub_label_per_image)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT6-mzwnIsYj",
        "outputId": "f1b30ceb-f81b-4a7c-baf0-0001d7d59d10"
      },
      "source": [
        "model_list = [\n",
        "    \"models/KB_wc_2500.ckpt\",\n",
        "    \"models/KB_nc_2500.ckpt\"]\n",
        "\n",
        "config = tf.ConfigProto(device_count={'GPU': 1})\n",
        "\n",
        "for model_type in model_list:\n",
        "\n",
        "    model = model_type\n",
        "    model_label = model.split(\"/\")[-1][:-5]\n",
        "    print(model.upper())\n",
        "    obj_bboxes_ours_output = []\n",
        "    sub_bboxes_ours_output = []\n",
        "    predicted_predicates_values_tensor = tf.concat([isInRelation[predicate].tensor() for predicate in selected_predicates], 1)\n",
        "    # concat edited, axis was in the wrong place\n",
        "    saver = tf.train.Saver()\n",
        "    sess = tf.Session(config=config)\n",
        "    saver.restore(sess, model)\n",
        "    rlp_confs_ours = []\n",
        "    rlp_labels_ours = []\n",
        "    error_an = np.array([]).reshape(0, 8)\n",
        "\n",
        "    for pic_idx in range(gt_tuple_label.shape[1]):\n",
        "        if pic_idx % 100 == 0:\n",
        "            print(\"Eval img\" + str(pic_idx))\n",
        "        values_of_predicates = np.array([], dtype=np.float32).reshape(0, 70)\n",
        "        values_of_predicates = sess.run(predicted_predicates_values_tensor, {pairs_of_objects.tensor: features_detected_bb[pic_idx]}) \n",
        "        values_of_predicates = refine_equiv(values_of_predicates, selected_predicates, \"max\")\n",
        "        values_of_predicates = np.multiply(values_of_predicates, prior_freq)\n",
        "\n",
        "\n",
        "        conf_predicates_per_image = values_of_predicates.flatten('F')\n",
        "        sub_bboxes_ours_output.append(np.tile(sub_bboxes_ours[pic_idx], (len(selected_predicates), 1)))\n",
        "        obj_bboxes_ours_output.append(np.tile(obj_bboxes_ours[pic_idx], (len(selected_predicates), 1)))\n",
        "        # Matlab indices start from 1\n",
        "        label_predicates_per_image = np.hstack((np.tile(sub_labels_ours[pic_idx], len(selected_predicates))[:, np.newaxis],\n",
        "                                                np.repeat(np.array(range(1, len(selected_predicates) + 1)), len(features_detected_bb[pic_idx]))[:, np.newaxis],\n",
        "                                                np.tile(obj_labels_ours[pic_idx], len(selected_predicates))[:, np.newaxis]))\n",
        "\n",
        "        rlp_confs_ours.append(conf_predicates_per_image[:, np.newaxis])\n",
        "        rlp_labels_ours.append(label_predicates_per_image)\n",
        "    sess.close()\n",
        "\n",
        "    sio.savemat(\"Visual-Relationship-Detection-master/results_LTN/predicate_det_result_\" + model_label + \".mat\",\n",
        "                {'sub_bboxes_ours': sub_bboxes_ours_output,\n",
        "                 'obj_bboxes_ours': obj_bboxes_ours_output,\n",
        "                 'rlp_confs_ours': rlp_confs_ours,\n",
        "                 'rlp_labels_ours': rlp_labels_ours})"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODELS/KB_WC_2500.CKPT\n",
            "INFO:tensorflow:Restoring parameters from models/KB_wc_2500.ckpt\n",
            "Eval img0\n",
            "Eval img100\n",
            "Eval img200\n",
            "Eval img300\n",
            "Eval img400\n",
            "Eval img500\n",
            "Eval img600\n",
            "Eval img700\n",
            "Eval img800\n",
            "Eval img900\n",
            "MODELS/KB_NC_2500.CKPT\n",
            "INFO:tensorflow:Restoring parameters from models/KB_nc_2500.ckpt\n",
            "Eval img0\n",
            "Eval img100\n",
            "Eval img200\n",
            "Eval img300\n",
            "Eval img400\n",
            "Eval img500\n",
            "Eval img600\n",
            "Eval img700\n",
            "Eval img800\n",
            "Eval img900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW5S-5EBMDYn",
        "outputId": "62e25200-e9d0-469c-ba41-20708eaaf916"
      },
      "source": [
        "tf.concat(1, [isInRelation[predicate].tensor() for predicate in selected_predicates])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'concat_8/concat:0' shape=() dtype=int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9EHWKakLeNX",
        "outputId": "bc7b68c6-c468-4aeb-d64a-4a2596efc213"
      },
      "source": [
        "tf.concat([isInRelation[predicate].tensor() for predicate in selected_predicates], 1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'concat_7:0' shape=<unknown> dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}